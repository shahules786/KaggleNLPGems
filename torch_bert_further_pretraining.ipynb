{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Further Pre-training\n\nBesides the training data of a target task, we can further pre-train a transformer on the data from the same domain.\n\n![image.png](https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-32381-3_16/MediaObjects/489562_1_En_16_Fig1_HTML.png)\n\nThe Transformer models are pre-trained on the general domain corpus. For a text classification task / regression task in a specific domain, such as Readability Assesment, its data\ndistribution may be different from a transformer trained on a different corpus e.g. RoBERTa trained on BookCorpus, Wiki, CC-News, OpenWebText, Stories. Therefore the idea is, we can further pre-train the transformer with masked language model and next sentence prediction tasks on the domain-specific data. Three further pretraining approaches are performed:\n\n1) `Within-task pre-training (ITPT)`, in which transformer is further pre-trained on the training data of a target task. `This Kernel.`\n\n2) `In-domain pre-training (IDPT)`, in which the pretraining data is obtained from the same domain of a target task. For example, there are several different sentiment classification tasks, which have a similar data distribution. We can further pre-train the transformer on the combined training data from these tasks.\n\n3) `Cross-domain pre-training (CDPT)`, in which the pretraining data is obtained from both the same and other different domains to a target task.\n\n#### Reference: [How to finetune BERT for Text Classification ?](https://arxiv.org/pdf/1905.05583.pdf)\n\n> Note: This Kernel implements ITPT i.e. Within-Task Pretraining. First we will pretrain a RoBERTa model and then utilize the same for further finetuing tasks using different strategies.","metadata":{}},{"cell_type":"markdown","source":"#### Code Reference: \n`Transformer Examples` - https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm_no_trainer.py\n\n> 90-95% of the code is from this great `run_mlm_no_trainer.py script` from `HuggingFace Examples Repository`. I have merely `changed few lines to adjust the code according to my task`. \n    \n    P.S. Make sure to understand everything instead of blindly copying the code.","metadata":{}},{"cell_type":"markdown","source":"### Install Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets accelerate ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-05-21T19:09:29.298545Z","iopub.execute_input":"2021-05-21T19:09:29.298883Z","iopub.status.idle":"2021-05-21T19:09:40.807034Z","shell.execute_reply.started":"2021-05-21T19:09:29.298854Z","shell.execute_reply":"2021-05-21T19:09:40.806229Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.4.2)\nCollecting datasets\n  Downloading datasets-1.6.2-py3-none-any.whl (221 kB)\n\u001b[K     |████████████████████████████████| 221 kB 413 kB/s eta 0:00:01\n\u001b[?25hCollecting accelerate\n  Downloading accelerate-0.3.0-py3-none-any.whl (49 kB)\n\u001b[K     |████████████████████████████████| 49 kB 1.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from accelerate) (1.7.0)\nRequirement already satisfied: pyaml>=20.4.0 in /opt/conda/lib/python3.7/site-packages (from accelerate) (20.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from pyaml>=20.4.0->accelerate) (5.3.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (1.19.5)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\nRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.0.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\nCollecting xxhash\n  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n\u001b[K     |████████████████████████████████| 243 kB 2.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (20.9)\nCollecting tqdm<4.50.0,>=4.27\n  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n\u001b[K     |████████████████████████████████| 69 kB 3.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.5)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\nCollecting huggingface-hub<0.1.0\n  Downloading huggingface_hub-0.0.9-py3-none-any.whl (37 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.1.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.11.13)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (2.4.7)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\nInstalling collected packages: tqdm, xxhash, huggingface-hub, datasets, accelerate\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.56.2\n    Uninstalling tqdm-4.56.2:\n      Successfully uninstalled tqdm-4.56.2\nSuccessfully installed accelerate-0.3.0 datasets-1.6.2 huggingface-hub-0.0.9 tqdm-4.49.0 xxhash-2.0.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Load CommonLit Readability Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n\nmlm_data = train[['excerpt']]\nmlm_data = mlm_data.rename(columns={'excerpt':'text'})\nmlm_data.to_csv('mlm_data.csv', index=False)\n\nmlm_data_val = test[['excerpt']]\nmlm_data_val = mlm_data_val.rename(columns={'excerpt':'text'})\nmlm_data_val.to_csv('mlm_data_val.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T19:09:40.810133Z","iopub.execute_input":"2021-05-21T19:09:40.810486Z","iopub.status.idle":"2021-05-21T19:09:41.276891Z","shell.execute_reply.started":"2021-05-21T19:09:40.810452Z","shell.execute_reply":"2021-05-21T19:09:41.276125Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Import Dependencies","metadata":{}},{"cell_type":"code","source":"import argparse\nimport logging\nimport math\nimport os\nimport random\n\nimport datasets\nfrom datasets import load_dataset\nfrom tqdm.auto import tqdm\nfrom accelerate import Accelerator\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nimport transformers\nfrom transformers import (\n    CONFIG_MAPPING, \n    MODEL_MAPPING, \n    AdamW, \n    AutoConfig, \n    AutoModelForMaskedLM, \n    AutoTokenizer, \n    DataCollatorForLanguageModeling, \n    SchedulerType, \n    get_scheduler, \n    set_seed\n)\n\nlogger = logging.getLogger(__name__)\nMODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n\n# from pprint import pprint\n# pprint(MODEL_TYPES, width=3, compact=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T19:09:41.279483Z","iopub.execute_input":"2021-05-21T19:09:41.279867Z","iopub.status.idle":"2021-05-21T19:09:48.552546Z","shell.execute_reply.started":"2021-05-21T19:09:41.279831Z","shell.execute_reply":"2021-05-21T19:09:48.551810Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Config","metadata":{}},{"cell_type":"code","source":"class TrainConfig:\n    train_file= 'mlm_data.csv'\n    validation_file = 'mlm_data.csv'\n    validation_split_percentage= 5\n    pad_to_max_length= True\n    model_name_or_path= 'roberta-base'\n    config_name= 'roberta-base'\n    tokenizer_name= 'roberta-base'\n    use_slow_tokenizer= True\n    per_device_train_batch_size= 8\n    per_device_eval_batch_size= 8\n    learning_rate= 5e-5\n    weight_decay= 0.0\n    num_train_epochs= 1 # change to 5\n    max_train_steps= None\n    gradient_accumulation_steps= 1\n    lr_scheduler_type= 'constant_with_warmup'\n    num_warmup_steps= 0\n    output_dir= 'output'\n    seed= 2021\n    model_type= 'roberta'\n    max_seq_length= None\n    line_by_line= False\n    preprocessing_num_workers= 4\n    overwrite_cache= True\n    mlm_probability= 0.15\n\nconfig = TrainConfig()\n\nif config.train_file is not None:\n    extension = config.train_file.split(\".\")[-1]\n    assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, json or txt file.\"\nif config.validation_file is not None:\n    extension = config.validation_file.split(\".\")[-1]\n    assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, json or txt file.\"\nif config.output_dir is not None:\n    os.makedirs(config.output_dir, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T19:09:48.553933Z","iopub.execute_input":"2021-05-21T19:09:48.554259Z","iopub.status.idle":"2021-05-21T19:09:48.566730Z","shell.execute_reply.started":"2021-05-21T19:09:48.554228Z","shell.execute_reply":"2021-05-21T19:09:48.565628Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Run","metadata":{}},{"cell_type":"code","source":"def main():\n    args = TrainConfig()\n    accelerator = Accelerator()\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state)\n    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    data_files = {}\n    if args.train_file is not None:\n        data_files[\"train\"] = args.train_file\n    if args.validation_file is not None:\n        data_files[\"validation\"] = args.validation_file\n    extension = args.train_file.split(\".\")[-1]\n    if extension == \"txt\":\n        extension = \"text\"\n    raw_datasets = load_dataset(extension, data_files=data_files)\n    \n    if args.config_name:\n        config = AutoConfig.from_pretrained(args.config_name)\n    elif config.model_name_or_path:\n        config = AutoConfig.from_pretrained(args.model_name_or_path)\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning(\"You are instantiating a new config instance from scratch.\")\n\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n    elif args.model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)\n    else:\n        raise ValueError(\n            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n        )\n    \n    if args.model_name_or_path:\n        model = AutoModelForMaskedLM.from_pretrained(\n            args.model_name_or_path,\n            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n            config=config,\n        )\n    else:\n        logger.info(\"Training new model from scratch\")\n        model = AutoModelForMaskedLM.from_config(config)\n\n    model.resize_token_embeddings(len(tokenizer))\n\n    column_names = raw_datasets[\"train\"].column_names\n    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n\n    if args.max_seq_length is None:\n        max_seq_length = tokenizer.model_max_length\n        if max_seq_length > 1024:\n            logger.warning(\n                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n                \"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"\n            )\n            max_seq_length = 1024\n    else:\n        if args.max_seq_length > tokenizer.model_max_length:\n            logger.warning(\n                f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the\"\n                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n            )\n        max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n\n    def tokenize_function(examples):\n        return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n\n    tokenized_datasets = raw_datasets.map(\n        tokenize_function,\n        batched=True,\n        num_proc=args.preprocessing_num_workers,\n        remove_columns=column_names,\n        load_from_cache_file=not args.overwrite_cache,\n    )\n\n    def group_texts(examples):\n        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\n        total_length = (total_length // max_seq_length) * max_seq_length\n        result = {\n            k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n            for k, t in concatenated_examples.items()\n        }\n        return result\n\n    tokenized_datasets = tokenized_datasets.map(\n        group_texts,\n        batched=True,\n        num_proc=args.preprocessing_num_workers,\n        load_from_cache_file=not args.overwrite_cache,\n    )\n    train_dataset = tokenized_datasets[\"train\"]\n    eval_dataset = tokenized_datasets[\"validation\"]\n\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=args.mlm_probability)\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n    )\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\n    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader\n    )\n\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    else:\n        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps,\n        num_training_steps=args.max_train_steps,\n    )\n\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n\n    for epoch in range(args.num_train_epochs):\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss = loss / args.gradient_accumulation_steps\n            accelerator.backward(loss)\n            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if completed_steps >= args.max_train_steps:\n                break\n\n        model.eval()\n        losses = []\n        for step, batch in enumerate(eval_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n\n            loss = outputs.loss\n            losses.append(accelerator.gather(loss.repeat(args.per_device_eval_batch_size)))\n\n        losses = torch.cat(losses)\n        losses = losses[: len(eval_dataset)]\n        perplexity = math.exp(torch.mean(losses))\n\n        logger.info(f\"epoch {epoch}: perplexity: {perplexity}\")\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T19:09:48.569141Z","iopub.execute_input":"2021-05-21T19:09:48.569506Z","iopub.status.idle":"2021-05-21T19:09:48.602210Z","shell.execute_reply.started":"2021-05-21T19:09:48.569469Z","shell.execute_reply":"2021-05-21T19:09:48.601616Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T19:09:48.604178Z","iopub.execute_input":"2021-05-21T19:09:48.604627Z","iopub.status.idle":"2021-05-21T19:13:34.776786Z","shell.execute_reply.started":"2021-05-21T19:09:48.604571Z","shell.execute_reply":"2021-05-21T19:13:34.775790Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-3a963151006315c5/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-3a963151006315c5/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n","output_type":"stream"},{"name":"stderr","text":"https://huggingface.co/roberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpt8xxi2hc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eff24bfe430f4e5d95eb00e2e4773dfa"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/roberta-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\ncreating metadata file for /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\nloading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\nModel config RobertaConfig {\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.4.2\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\nModel config RobertaConfig {\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.4.2\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\nhttps://huggingface.co/roberta-base/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi_x_dk68\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10d1b9c6b4c54db6b9b42c68d37e0f9a"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/roberta-base/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\ncreating metadata file for /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"https://huggingface.co/roberta-base/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpexuuqcrp\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42bd05cef6a54c0f93cce994b7168a09"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/roberta-base/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\ncreating metadata file for /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"https://huggingface.co/roberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6k33rca8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355863.0, style=ProgressStyle(descript…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60fb3b1a74c74a15a5ffae6dcebfbcda"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/roberta-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\ncreating metadata file for /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\nloading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\nloading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\nloading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\nloading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpm03e_l0o\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=501200538.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99f40c14c7624dafa44528f7eda867f4"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\ncreating metadata file for /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\nloading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint weights were used when initializing RobertaForMaskedLM.\n\nAll the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=151.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e13c1840e2354a17ac21bf85a6c90421"}},"metadata":{}},{"name":"stderr","text":"Configuration saved in output/config.json\nModel weights saved in output/pytorch_model.bin\n","output_type":"stream"}]}]}